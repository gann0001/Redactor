Project Title: Redactor
Project Description:
The main goal of this project is to hide the sensitive information like names, genders, dates, places, addresses, phones in a text file. because while disclosing the information to the public the important, restricted, sensitive information is supposed to be not disclosable. 
Installation Instructions for Phase I and Phase II:

Phase I:
1. The project is compressed as tar.gz file. untar the file (tar -xvf compressedfilename.
2. pip3 install -r requirements.txt to install all the requirements
3. install the code : pip3 install --editable .
4. Download StanfordNer usin command in "redactor" location: wget https://nlp.stanford.edu/software/stanford-ner-2016-10-31.zip
5. unzip the file:  unzip stanford-ner-2016-10-31.zip
6. copy stanford-ner-2016-10-31.zip to three different locations i) tests/ ii) redactor/(downloaded stanford to this location) iii)redactor/redactor/
7. execute the redactor.py python file
8. use "python3 setup.py test" execute tests using the pytest-runner
Ex: python3 redactor.py --input '*.html' --input 'otherfiles/*.txt' --names --dates --places --genders --concept 'kids' --addresses --phones --output 'files/' --stats stdout

Conditions & Assumptions:    
1. --output 'files/' must be specified in the execution otherwise program doesn't know where to store redacted file so it throws an error.
2. To execute the redactor.py multiple times, we need to delete the output directory(files/) otherwise warning message will be printed in console.
      mkdir: cannot create directory ‘files’: File exists
3. I added one sample  html and  text file in  the locations redactor/redactor/ and redactor/redactor/otherfiles/ respectively for the execution. 
4. Order of the arguments doesn’t matter as I have written code such a way that I have created my own order of execution 
Structure:
This Project is created in structured format as like below where redactor.py contains all the functions of the project where it will be used to execute the code and Total code is reside inside the redactor.py file. Here setup.py describes the code package and requirements.txt describe all external packages. 

redactor/
	redactor/
		__init__.py
		redactor.py
		a.html
		unredactor.py
		otherfiles/
		train/
			*.txt
		test/
			*.txt
	tests/
		test_names.py
		test_places.py
		test_vectors.py
		test_train_vector.py
		test_train_dict.py
		test_dict.py
		test_dates.py
		test_phones.py
		test_genders.py
		test_address.py
	docs/
	README
	requirements.txt
	setup.cfg
	setup.py
	

Functions:
Below are the main functions used in the project.
--> main()
main function is to handle the command line arguments which accept the text and html files. There are 6 different functions in order to handle the sensitive informations. These functions will look into sensitive information(names,places,addresses,dates,phones,concept,gender) and return to the main function.

(i)names(text): 
By using nltk ne_chunk method with binary = False which dispays all PERSONS, ORGANIZATION, GPE as a label for names, organizations and GPE. All the words or bag of words which is having label 'PERSON' is added to the List. I have parsed the list to create a dictionary which contains  name, starting index and ending index.
(ii)places(text):
This function takes the text and returns all the place list. I have parsed the list and created dictionary which contains place, starting index and ending index.  I used Stanford NER to detect all the places and I have written grammar for Locations which is having two tokens like (San Fransisco).

(iii)concept(text): 
This function accept the file content as arguments and look for information related to concept and it returns concepts list (It contains all concept sentences in a file) 

For Example: In command line argument if it is given as --input 'Prison'. It will look for all the word related to prison i.e. synonyms. Sentence which containing the word or related words(synonyms), that whole sentences will be redacted. 

(iv)address(text): I have used regular expressions to detect all addresses in file. This functions takes the text and find most of the addresses and returns a list (which contains all the addresses)
	Assumption: Standard Address should contain two (,) in between the address. 
EX: 1003 E Brook Street, APT A, OK 73071 

(v)phones(text): 
The regular expression look for the all phone numbers in the text file and handling most of the standard US phone numbers. This function takes text and returns a list which contains most of the addresses 
Ex: 9723457654 , (405)-464-2839 , 405.464.2839 , 123-456-7890 , (123)-456-7890 , 123 456 7890 , (223)456 7891 x 123 , (223)456 7891 ext 123

(vi)dates(text): 
The regular expression look for different formats of dates in the text file and handles most of the cases. This function takes text as input and returns the list which contains all the different type of dates.
Redaction:
After collecting all lists and dictionaries I have created different functions to redact the text. Below are the functions for redaction process.

Handling CommandLine Arguments:
* The readactor.py python file will accept the all arguments which are given in runtime and basically it takes the html and text files to parse.
* Arguments given in command line in any order, I have created my own order of execution because in order to handle redaction process smoothly 
	Ex: If the --place is given in command line arguments first than address so, identity the place in addresses and redact it. With my own order of execution it works perfectly. The order is(concept, address, phones, dates, names, places, genders)  

Stats:
If it is --stats stdout/ --> Prints the a summary of the redaction process in console and It prints redacted items and counts of redacted items. 
Log files will be created for stats in files/ location if the arguments for stats are other than above
Challenges Faced:

•	While extracting the names and places using NLTK ne_chunk and pos_tag are not providng accurate data. It seems StanfordNer is better than NLTK. so I used StanforNER for extracting all the places.
•	Extracting all different types of dates is pretty difficult I tried to handle many cases with regular expressions and I’ve tried datefinder module which is not appropriate.
•	Usage and understanding of Stanford NER and writing grammar for combining two tokenis like San and Francisco. I’ve used it and It is working very well.

Note : In only 1% case, if the file is very unstructured, stanford NER will also not give accurate data so in that case redaction.py file may throw an error because I'm using dictionary to store names and places and their indexes. Index may fail in that case. 
PDF Creation:
All the output redacted files will store in files/ and cupsfilter is used to convert it to pdf

Pytest:
I have written ten test files to do some unittesing on my code for redactor and unredactor.
Observation:
NLTK and NLTK methods are not giving accurate information to hide the sensitive information. 

Phase2:
Project Title: UnRedactor
Project Description:
Unredactor will take a name redacted file as input. The main goal of this project is to predict the name which is redacted.
Installation Instructions for and Phase II:
1.	Unredactor.py python file is in redactor/redactor location.
2.	Execute python3 unredactor.py 
3.	There is no need to give command line arguments for training and testing because I have added train and test folders for training input files and test input files respectively (handled the code)

Functions:
(a) entities(list_data)
	This fuction will take the list. each element of a list is one file data and it returns all the entities of all files and dictionary with features for each entity.
(b) test(data)
	This function will take content of test file and returns the list and dictionary. list contains all candidate entities and dictionary contains features associated with that entity. 
Procedure followed:
Training: 
•	List of all the training files are in training/*.txt 
•	I would recommend adding few more test files in location:  train/
•	training() and entities() functions collect all the entities in the training files and 
•	For each entity, I have created a feature vector associated with that entity.  Below are the features.
	1. Entity name
	2. Length of name
	3. Space count of name
	4. 1 gram count of name
	5. 2 gram count of name
	6. 3 gram count of name
	7. Length of first token in name
•	To get feature vector for each entity, I have created a dictionary with all above features.
•	I’ve passed the dictionary to DictVectorizer() and used fit_transfrom. The "fit" part applies to the feature extractor itself: it determines what features it will base future transformations on. The "transform" part is what takes the data and spits some transformed data back at you.
•	In order to train the data, I used several algorithms namely GussianNB, MultinomialNB, BernoulliNB and K-Nearest Neighbours. 
•	Training of any of these algorithms take two parameters as input(trainvectors, total names in the same order)

Testing:
•	Training files can be added in train/ location to predict the redacted names
•	While testing or predicting the data. For each file, I have created a test function to extracted all candidate entities.
•	For each entity, I used the same features which are used for training to create feature vectors. This condition must be satisfied otherwise we may get dimension error.
•	With the same features, I have created a dictionary and passed to the dictionary to dictvectorizer.
•	For each file I’m printing predictions for GussianNB and K-Nearest Neighbours (this will give the n=5 near points ).
•	
Assumptions & Conditions:
•	In Testing, candidate entity in input file should have spaces when the candidate entity is having multiple tokens. 
	EX: ****** *******(Ashton Kutcher)  
•	I have tested unredactor several times with different algorithms (MultinomialNB, GussianNB, BernoulliNB,, K Nearest Neighbors). Finally I concluded that, GussianNB, K Nearest Neighbors prediction is better.
•	I have trained my model  3 times with same data in order to get some good prediction.


Challenges Faced:
•	Prediction accurate is low. I tried to increase the performance by using several algorithms like MultinomialNB, GussianNB, MultinomialNB, K Nearest Neighbors. 
•	I tried to add sentences as a feature and sentence length. I was able to add it for training i faced problem adding sentence for candidate entity but it decreased the performance so i removed those feature from my list.
Observations:
•	If the same entity appeared many times in the training files during training. There are more chances to get that as prediction if it matches length and space because those are the major features in the training.
•	According to my analysis, GussianNB algorithm and next K-Nearest neighbor’s algorithm are bettern than MultinomialNB, BernoulliNB.


References used to complete this project:
http://www.nltk.org/book/ch07.html
https://nlp.stanford.edu/software/CRF-NER.shtml
https://github.com/openeventdata/phoenix_pipeline/blob/master/timex.py
https://www.tutorialspoint.com/python/python_command_line_arguments.htm

